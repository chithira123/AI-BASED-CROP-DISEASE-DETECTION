{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtqWknMo3dmhiylLAORHNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chithira123/AI-BASED-CROP-DISEASE-DETECTION/blob/main/text_data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIloir5wfT9t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize , sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk. stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punk')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "file_path = \"/content/nlp_dataset (1).csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "print(df)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def process_text (text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_tokens = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
        "  return {\n",
        "      \"Original_Text\":text,\n",
        "      \"Word_Tokens\": word_tokens,\n",
        "      \"Filtered_Tokens\": filtered_tokens,\n",
        "      \"Lemmatized_Tokens\": lemmatized_tokens,\n",
        "\n",
        "  }\n",
        "  results = df['Text'].apply(process_text)\n",
        "  processed_df = pd.DataFrame(results.tolist())\n",
        "  print(processed_df.head())\n",
        "  processed_file_path = \"procedded_nlpdataset,csv\"\n",
        "  processed_df.to_csv(procedded_file_path, index= False)\n",
        "  print(f\"processed dataset saved to {procedded_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwyNBMkFhK4U",
        "outputId": "7446be82-cbfb-4db7-fca9-5c32b02adb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Text\n",
            "0  Natural Language Processing is a fascinating f...\n",
            "1  It bridges the gap between humans and machines...\n",
            "2  NLP techniques are widely used in applications...\n",
            "3  Machine translation and speech recognition are...\n",
            "4  Despite its advancements, NLP faces challenges...\n",
            "5  Continuous research and innovation are improvi...\n",
            "6  The potential of NLP is vast, making it essent...\n",
            "{'on', \"you're\", 'theirs', \"won't\", \"you'd\", 'are', 'we', 'should', 'that', 'just', \"i'll\", 'd', 'any', 'while', \"it'd\", \"couldn't\", \"they're\", 'doesn', 'been', \"should've\", \"they've\", 'your', 'it', 'off', 'each', 're', \"didn't\", \"hadn't\", \"don't\", \"he's\", 'itself', 'under', 'had', \"that'll\", 'yours', 'me', 'or', 'himself', \"weren't\", \"mustn't\", 'don', 'such', 'she', 'how', 'those', \"hasn't\", 'very', 'only', 'about', 'why', \"i've\", 'wouldn', 'other', 'all', \"i'd\", 'same', 'below', 'has', 'ma', \"we're\", 'if', 'most', 'our', 'against', 'am', 'during', 'you', \"mightn't\", 'the', 'because', 'as', 'do', 'yourself', 'once', 'some', 'hadn', 'shouldn', 'being', 'ours', \"we've\", \"she's\", \"needn't\", 'further', 'few', 'he', 'through', 'ourselves', 'here', \"aren't\", 'so', 'above', 'does', \"she'll\", 'to', 'in', 'this', 'too', 'until', 'what', 'who', 'a', 'they', 'his', 'again', 'mightn', 'll', \"she'd\", 'own', 'now', \"shouldn't\", 'an', 'which', 'can', \"you'll\", 'doing', 'ain', 'them', 'were', 've', 'won', 'up', \"shan't\", 'whom', \"he'll\", 's', 't', 'isn', 'by', 'these', \"you've\", 'did', 'then', 'm', \"we'll\", 'nor', \"i'm\", \"wasn't\", 'into', 'having', 'will', 'but', 'hasn', 'after', \"haven't\", \"doesn't\", 'him', 'y', \"they'll\", \"they'd\", 'shan', 'my', \"he'd\", 'weren', 'before', 'at', 'not', \"wouldn't\", 'between', 'than', 'hers', 'have', 'wasn', 'its', 'more', 'and', 'her', 'didn', 'over', \"we'd\", 'from', 'was', 'where', 'down', 'for', 'themselves', 'aren', 'couldn', 'i', 'there', \"it'll\", 'myself', 'haven', 'mustn', 'when', 'herself', 'no', 'is', 'their', 'needn', 'o', 'out', \"isn't\", 'with', 'both', \"it's\", 'yourselves', 'be', 'of'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading punk: Package 'punk' not found in index\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wTZc9ZDjmGZ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}